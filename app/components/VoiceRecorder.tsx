// app/components/VoiceRecorder.tsx - Enhanced voice recorder with direct Dialogflow audio support
'use client';

import React, { useState, useRef, useCallback } from 'react';

interface VoiceRecorderProps {
  onAudioResult: (result: { text: string; source: 'speech-recognition' | 'dialogflow-audio' }) => void;
  sessionId: string;
  disabled?: boolean;
}

export default function VoiceRecorder({ onAudioResult, sessionId, disabled }: VoiceRecorderProps) {
  const [isRecording, setIsRecording] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  const [recordingMethod, setRecordingMethod] = useState<'speech-recognition' | 'dialogflow-audio'>('speech-recognition');
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const recognitionRef = useRef<any>(null);

  // Initialize speech recognition for browser-based voice recognition
  const initializeSpeechRecognition = useCallback(() => {
    if (typeof window === 'undefined') return null;

    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    if (!SpeechRecognition) return null;

    const recognition = new SpeechRecognition();
    recognition.continuous = false;
    recognition.interimResults = false;
    recognition.lang = 'el-GR'; // Greek language
    recognition.maxAlternatives = 1;

    recognition.onresult = (event: any) => {
      const transcript = event.results[0][0].transcript;
      console.log('ğŸ—£ï¸ Speech Recognition Result:', transcript);
      onAudioResult({ text: transcript, source: 'speech-recognition' });
      setIsRecording(false);
      setIsProcessing(false);
    };

    recognition.onerror = (event: any) => {
      console.error('âŒ Speech Recognition Error:', event.error);
      setIsRecording(false);
      setIsProcessing(false);
    };

    recognition.onend = () => {
      setIsRecording(false);
      setIsProcessing(false);
    };

    return recognition;
  }, [onAudioResult]);

  // Convert audio blob to base64
  const audioToBase64 = (audioBlob: Blob): Promise<string> => {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onload = () => {
        const result = reader.result as string;
        const base64 = result.split(',')[1];
        resolve(base64);
      };
      reader.onerror = () => reject(new Error('Failed to convert audio to base64'));
      reader.readAsDataURL(audioBlob);
    });
  };

  // Send audio directly to Dialogflow
  const sendAudioToDialogflow = async (audioBlob: Blob) => {
    try {
      setIsProcessing(true);
      console.log('ğŸ¤ Sending audio to Dialogflow...');

      const base64Audio = await audioToBase64(audioBlob);
      
      const response = await fetch('/api/audio-chat', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          sessionId,
          inputAudio: base64Audio,
          sampleRate: 16000,
          languageCode: 'el'
        }),
      });

      if (!response.ok) {
        const error = await response.json();
        throw new Error(error.error || 'Failed to process audio');
      }

      const result = await response.json();
      console.log('âœ… Dialogflow Audio Result:', result);
      
      // Return the Dialogflow response text
      onAudioResult({ text: result.response, source: 'dialogflow-audio' });
    } catch (error: any) {
      console.error('âŒ Dialogflow Audio Error:', error);
      onAudioResult({ 
        text: `Î£Ï…Î³Î³Î½ÏÎ¼Î·, Ï…Ï€Î®ÏÎ¾Îµ ÏƒÏ†Î¬Î»Î¼Î± ÎºÎ±Ï„Î¬ Ï„Î·Î½ ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± Ï„Î¿Ï… Î®Ï‡Î¿Ï…: ${error.message}`, 
        source: 'dialogflow-audio' 
      });
    } finally {
      setIsProcessing(false);
    }
  };

  // Start recording
  const startRecording = async () => {
    try {
      setIsRecording(true);
      audioChunksRef.current = [];

      if (recordingMethod === 'speech-recognition') {
        // Use browser speech recognition
        recognitionRef.current = initializeSpeechRecognition();
        if (recognitionRef.current) {
          recognitionRef.current.start();
          console.log('ğŸ¤ Started Speech Recognition');
        } else {
          throw new Error('Speech recognition not supported');
        }
      } else {
        // Use MediaRecorder for Dialogflow audio
        const stream = await navigator.mediaDevices.getUserMedia({ 
          audio: {
            sampleRate: 16000,
            channelCount: 1,
            echoCancellation: true,
            noiseSuppression: true
          } 
        });

        mediaRecorderRef.current = new MediaRecorder(stream, {
          mimeType: 'audio/webm;codecs=opus'
        });

        mediaRecorderRef.current.ondataavailable = (event) => {
          if (event.data.size > 0) {
            audioChunksRef.current.push(event.data);
          }
        };

        mediaRecorderRef.current.onstop = () => {
          const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm;codecs=opus' });
          sendAudioToDialogflow(audioBlob);
          
          // Stop all tracks
          stream.getTracks().forEach(track => track.stop());
        };

        mediaRecorderRef.current.start();
        console.log('ğŸ¤ Started MediaRecorder for Dialogflow');
      }
    } catch (error: any) {
      console.error('âŒ Failed to start recording:', error);
      setIsRecording(false);
      onAudioResult({ 
        text: `Î£Ï…Î³Î³Î½ÏÎ¼Î·, Î´ÎµÎ½ Î¼Ï€ÏŒÏÎµÏƒÎ± Î½Î± Î¾ÎµÎºÎ¹Î½Î®ÏƒÏ‰ Ï„Î·Î½ Î·Ï‡Î¿Î³ÏÎ¬Ï†Î·ÏƒÎ·: ${error.message}`, 
        source: recordingMethod 
      });
    }
  };

  // Stop recording
  const stopRecording = () => {
    if (recordingMethod === 'speech-recognition' && recognitionRef.current) {
      recognitionRef.current.stop();
      console.log('ğŸ›‘ Stopped Speech Recognition');
    } else if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
      mediaRecorderRef.current.stop();
      console.log('ğŸ›‘ Stopped MediaRecorder');
    }
    setIsRecording(false);
  };

  return (
    <div className="voice-recorder flex flex-col items-center space-y-4">
      {/* Recording Method Selector */}
      <div className="flex space-x-4">
        <label className="flex items-center space-x-2">
          <input
            type="radio"
            value="speech-recognition"
            checked={recordingMethod === 'speech-recognition'}
            onChange={(e) => setRecordingMethod(e.target.value as 'speech-recognition')}
            disabled={isRecording || disabled}
            className="text-blue-600"
          />
          <span className="text-sm">Browser Speech Recognition</span>
        </label>
        <label className="flex items-center space-x-2">
          <input
            type="radio"
            value="dialogflow-audio"
            checked={recordingMethod === 'dialogflow-audio'}
            onChange={(e) => setRecordingMethod(e.target.value as 'dialogflow-audio')}
            disabled={isRecording || disabled}
            className="text-blue-600"
          />
          <span className="text-sm">Dialogflow Audio</span>
        </label>
      </div>

      {/* Recording Button */}
      <button
        onMouseDown={startRecording}
        onMouseUp={stopRecording}
        onTouchStart={startRecording}
        onTouchEnd={stopRecording}
        disabled={disabled || isProcessing}
        className={`
          px-6 py-3 rounded-full font-medium transition-all duration-200
          ${isRecording 
            ? 'bg-red-500 text-white shadow-lg animate-pulse' 
            : isProcessing
            ? 'bg-yellow-500 text-white'
            : 'bg-blue-500 text-white hover:bg-blue-600'
          }
          ${disabled ? 'opacity-50 cursor-not-allowed' : 'cursor-pointer'}
        `}
      >
        {isProcessing 
          ? 'â³ Î•Ï€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î±...' 
          : isRecording 
          ? 'ğŸ”´ Î—Ï‡Î¿Î³ÏÎ¬Ï†Î·ÏƒÎ·...' 
          : 'ğŸ¤ ÎšÏÎ±Ï„Î®ÏƒÏ„Îµ Î³Î¹Î± Î¿Î¼Î¹Î»Î¯Î±'
        }
      </button>

      {/* Instructions */}
      <div className="text-xs text-gray-600 text-center max-w-xs">
        <p>
          <strong>{recordingMethod === 'speech-recognition' ? 'Browser Recognition:' : 'Dialogflow Audio:'}</strong>
        </p>
        <p>
          {recordingMethod === 'speech-recognition' 
            ? 'ÎšÏÎ±Ï„Î®ÏƒÏ„Îµ Ï€Î±Ï„Î·Î¼Î­Î½Î¿ Ï„Î¿ ÎºÎ¿Ï…Î¼Ï€Î¯ ÎºÎ±Î¹ Î¼Î¹Î»Î®ÏƒÏ„Îµ. Î— Î±Î½Î±Î³Î½ÏÏÎ¹ÏƒÎ· Î³Î¯Î½ÎµÏ„Î±Î¹ ÏƒÏ„Î¿Î½ browser.'
            : 'ÎšÏÎ±Ï„Î®ÏƒÏ„Îµ Ï€Î±Ï„Î·Î¼Î­Î½Î¿ Ï„Î¿ ÎºÎ¿Ï…Î¼Ï€Î¯ ÎºÎ±Î¹ Î¼Î¹Î»Î®ÏƒÏ„Îµ. ÎŸ Î®Ï‡Î¿Ï‚ ÏƒÏ„Î­Î»Î½ÎµÏ„Î±Î¹ ÏƒÏ„Î¿ Dialogflow.'
          }
        </p>
      </div>

      {/* Status Indicator */}
      {(isRecording || isProcessing) && (
        <div className="flex items-center space-x-2 text-sm">
          <div className={`w-3 h-3 rounded-full ${isRecording ? 'bg-red-500 animate-pulse' : 'bg-yellow-500 animate-spin'}`}></div>
          <span>
            {isRecording 
              ? (recordingMethod === 'speech-recognition' ? 'Î‘ÎºÎ¿ÏÏ‰...' : 'Î—Ï‡Î¿Î³ÏÎ±Ï†Ï...')
              : 'Î•Ï€ÎµÎ¾ÎµÏÎ³Î¬Î¶Î¿Î¼Î±Î¹...'
            }
          </span>
        </div>
      )}
    </div>
  );
}
